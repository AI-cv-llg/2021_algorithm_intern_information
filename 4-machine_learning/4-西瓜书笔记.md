- [第 2 章 模型评估与选择](#第-2-章-模型评估与选择)
  - [2.1 经验误差与过拟合](#21-经验误差与过拟合)
  - [2.2 评估方法](#22-评估方法)
    - [2.2.1 留出法](#221-留出法)
    - [2.2.2 交查验证法](#222-交查验证法)
    - [2.2.3 自助法](#223-自助法)
    - [2.2.4 调参与最终模型](#224-调参与最终模型)
  - [2.3 性能度量](#23-性能度量)
    - [2.3.1 错误率与精度](#231-错误率与精度)
    - [2.3.2 查准率、查全率与F1](#232-查准率查全率与f1)
    - [2.3.3 ROC 与 AUC](#233-roc-与-auc)
  - [2.5 偏差与方差](#25-偏差与方差)
- [第 9 章 聚类](#第-9-章-聚类)
  - [9.1 聚类任务](#91-聚类任务)
  - [9.2 性能度量](#92-性能度量)
  - [9.3 距离计算](#93-距离计算)
  - [9.4 原型聚类](#94-原型聚类)
    - [9.4.1 k 均值算法](#941-k-均值算法)
- [参考资料](#参考资料)

## 第 2 章 模型评估与选择
### 2.1 经验误差与过拟合

+ 精度：精度=1-错误率。如果在 $m$ 个样本中有 $a$ 个样本分类错误，则错误率 $E=a/m$，精度 = $1-a/m$。
+ **误差**：一般我们把学习器的实际预测输出与样本的真实输出之间的差异称为“误差”（`error`）。学习器在训练集上的误差称为“训练误差”（`training error`），在**新样本**上的误差称为“泛化误差”（`generalization error`）。
+ “**过拟合**：学习器把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，从而导致泛化性能下降，这种现象称为”过拟合“（`overfitting`）。过拟合是机器学习算法面临的一个关键问题。
+ **欠拟合**：和过拟合想法，指的是学习器对训练样本的一般性质都未学号。欠拟合比较容器解决，在决策树中增加分治、在神经网络学习中学习训练轮数（`Epoch`）等方法都是有效的。

好的学习器应该尽可能学出适用于所有潜在样本的”普遍规律“。由于事先无法知道新样本是什么样子，所以无法直接获得泛化误差，同时训练误差又由于过拟合现象的存在而不适合作为标准，那么现实中如何进行模型评估与选择就是一个重要的问题了。

### 2.2 评估方法

通常使用一个测试集来评估学习器对新样本的判别能力，把测试集上的”测试误差“（`testing error`）作为泛化误差的近似。值得注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现（学习器之前没有见到过）。

对于一个包含 $m$ 个样本的数据集 $D = {(x_1, y_1)}, (x_2, y_2),...,(x_m, y_m)$，将其划分为训练集 $S$ 和测试集 $T$，有两种常见的方法：**留出法和交叉验证法**。

#### 2.2.1 留出法

”留出法“（`hold-out`） 直接将数据集 $D$ 划分为两个户次互斥的集合，一个集合作为训练集 $S$，另一个作为测试集 $T$，即 $D = S \cup T$，$S \cap T=\empty$。值得注意的是，训练/测试集的划分应该尽可能保持数据分布的一致性，避免数据划分过程引入额外的偏差而对最终结果产生影响。从采样(`sampling`)的角度来看数据集的划分过程，则保留类别比例的采样方式称为”分层采样“（`stratified sampling`）。例如数据集 $D$ 有 `1000` 个样本，`800` 个正样本，`200` 个负样本，将 `70%` 的样本作为训练集，`30%` 的样本作为测试集。考虑分层采样，则训练 $S$ 集的构成为：正样本 $= 1000\times 70\% \times (800/1000) = 560$，负样本 $=1000\times 70\% \times (200/1000) = 140$，同理可得测试集 `T` 包含 `240` 个正样本，`60` 个负样本。

另一个问题是即使在给定训练集/测试集样本比例后，依然存在多种划分方式对样本 $D$ 进行分割，例如前 `280` 个正样本和后 `280` 个正样本构建的训练集是不一样的。因此，单次使用留出法得到的的估计结果往往不可靠，在使用留出法时，一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。

#### 2.2.2 交查验证法

”交叉验证法“（`cross validation`）先将数据集 $D$ 划分为 $k$ 个大小相似的**互斥子集**，即 $D = D_1\cup D_2\cup...\cup D_k，D_i \cap D_j = \varnothing$。同时每个子集都应该尽可能保持**数据分布的一致性**，即类别比例相等，从 $D$ 中通过分层采样得到。然后，每次用 $k-1$ 个子集的并集作为训练集，剩下的一个子集作为测试集，这样可获得 $k$ 组训练集/测试集，从而可进行 $k$ 组训练和测试，把 $k$ 次测试结果取平均后返回。交叉验证法评估结果的稳定性和保真性在很大程度上取决于 $k$ 的取值，为了强调这一点，通常把交叉验证法称为“$k$ 折交叉验证”（`k-fold cross validation`）。$k$ 的常用取值分别是 `10、5、20` 等。图 `2.2` 给出了 `10` 折交叉验证的示意图。

![10折交叉验证示意图](../data/images/10-fold_cross_validation.png)

“10 次 10 折交叉验证法”与 “100 次留出法“都是进行了 `100` 次训练/测试。

交叉验证法的一个特例是留一法（`Leave-One-Out`，简称 `LDO`），留一法不受随机样本划分方式的影响，因为 $m$ 个样本只有唯一的划分方式划分为 $m$ 个子集-每个子集包含一个样本。虽然留一法的评估结果往往被认为比较准确，但是训练开销简直太大了，真实项目中很少见到有人这样用，而且留一法的估计结果未必永远比其他评估方法准确，毕竟“天下没有免费的午餐”。

#### 2.2.3 自助法

“自助法”（`bootstrapping`） ：给定包含 $m$ 个样本的数据集 $D$，对它进行采样产生数据集 ${D}'$：每次随机从 $D$ 中挑选一个样本，将其拷贝放入 ${D}'$，然后再将其放回 $D$ 中，下次采样依然可能被采样到；这个过程重复 $m$ 次，就得到了包含 $m$ 个样本的数据集 ${D}'$。显然，$D$ 中有一部分样本会在 ${D}'$ 中多次出现，另外一部分样本不出现。做一个简单估计，样本在 $m$ 次采样中始终不被采到的概率是 $(1-\frac{1}{m})^m$，取极限得到
$$(1-\frac{1}{m})^m \rightarrow \frac{1}{e}\approx 0.368$$

即通过自助采样，初始数据集 $D$ 中约有 `36.8%` 的样本未出现在采样数据集 ${D}'$ 中。

自助法只适用于数据集较小、难以有效划分训练/测试集的情况。值得注意的是，自助法产生的数据集改变了初始数据集的分布，这回引入估计偏差。

#### 2.2.4 调参与最终模型

需要认为设定的参数称为超参数，模型训练无法优化它的。现实当中常对每个参数选定一个范围和变化补偿，例如在 `[0,0.2]` 范围内以 `0.05` 为补偿，要评估的的候选参数有 `5` 个最终选定的参数从这 `5` 个候选值中产生，结果也许不是最佳值，但这是计算开销和性能估计之间进行折中的结果。

在模型评估与选择过程中由于需要流出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因此，在模型选择完成后，学习算法和参数配置已定，此使应该用数据集 $D$ 重新训练模型。这个模型在训练过程中使用了**所有 $m$ 个训练样本**，这个模型也是最终提交给用户的模型。

另外，值得注意的是，我们通常把学得模型在实际使用过程中遇到的数据集称为测试数据，为了加以区分，前面讲到的模型评估与选择中用于评估测试的数据常称为“验证集（`validation set`）”。

在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。

### 2.3 性能度量

对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要衡量模型泛化能力的评价标准，这就是性能度量（`performance measure`）。

在预测任务中，给定样本集 $D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$，其中 $y_i$ 是示例 $x_i$ 的真实标签。要评估学习器 $f$ 的性能，需要把学习器预测结果 $f(x)$ 与真实标签 $y$ 进行比较。

回归任务最常用的性能度量是“均方误差”（`mean squared error`）。
$$E(f;D) = \frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2$$

#### 2.3.1 错误率与精度

+ 错误率：分类错误的样本数占样本总数的比例；
+ 精度：分类正确样本的样本数占样本总数的比例。

错误率和精度是分类任务中最常用的两种性能度量，适用于二分类，也适用于多分类任务。分类错误率和精度的定义如下：
$$E(f;D) = \frac{1}{m}\sum_{i=1}^{m}(f(x_i) \neq y_i)^2$$
$$acc(f;D) = \frac{1}{m}\sum_{i=1}^{m}(f(x_i) = y_i)^2 = 1 - E(f;D)$$

#### 2.3.2 查准率、查全率与F1

错误率和精度虽然常用，但是并不能满足所有任务需求。比如以西瓜问题为例，假设瓜农拉来一车西瓜，我们用训练好的模型对西瓜进行判别，现如精度只能衡量有多少比例的西瓜被我们判断类别正确（两类：好瓜、坏瓜）。但是若我们更加关心的是“挑出的西瓜中有多少比例是好瓜”，或者”所有好瓜中有多少比例被挑出来“，那么精度和错误率这个指标显然是不够用的。

对于**二分类**问题，可将样例根据真实类别与学习器预测类别的组合划分为真正例（`true positive`）、假正例（`false positive`）、真反例（`true negative`）、假反例（`false negative`）四种情况，令 $TP、FP、TN、FN$ 分别表示其对应的样例数，显然有 $TP+FP+TN+FN = $ 样例总数。分类结果的”混淆矩阵“（`confusion matrix`）如下表所示。

![混淆矩阵](../data/images/混淆矩阵.png)

查准率（精确率） $P$ 与查全率（召回率） $R$ 分别定义为：

$$P = \frac{TP}{TP+FP}$$
$$R = \frac{TP}{TP+FN}$$

查准率和查全率是一对矛盾的的度量。一般来说，查全率高时，查准率往往偏低；而查全率高时，查准率往往偏低。通常只有在一些简单任务中，才可能使查全率和查准率都很好高。

精准率和召回率的关系可以用一个 `P-R` 图来展示，以查准率 `P` 为纵轴、查全率 `R` 为横轴作图，就得到了查准率－查全率曲线，简称 **P-R** 曲线，`PR` 曲线下的面积定义为 `AP`。

![PR曲线图](../data/images/ml/PR曲线.png)
> 为了绘图方便和美观，示意图显示出单调平滑曲线，但现实任务中的 `P-R` 曲线是非单调、不平滑的，且在很多局部有上下波动。

**如何理解 P-R 曲线呢**？

> 可以从排序型模型或者分类模型理解。以逻辑回归举例，逻辑回归的输出是一个 `0` 到 `1` 之间的概率数字，因此，如果我们想要根据这个概率判断用户好坏的话，我们就必须定义一个阈值 。通常来讲，逻辑回归的概率越大说明越接近 `1`，也就可以说他是坏用户的可能性更大。比如，我们定义了阈值为 `0.5`，即概率小于 `0.5` 的我们都认为是好用户，而大于 `0.5` 都认为是坏用户。因此，对于阈值为 `0.5` 的情况下，我们可以得到相应的**一对**查准率和查全率。
但问题是：这个阈值是我们随便定义的，我们并不知道这个阈值是否符合我们的要求。 因此，为了找到一个最合适的阈值满足我们的要求，我们就必须遍历 `0` 到 `1` 之间所有的阈值，而每个阈值下都对应着一对查准率和查全率，从而我们就得到了 `PR` 曲线。
最后如何找到最好的阈值点呢？ 首先，需要说明的是我们对于这两个指标的要求：我们希望查准率和查全率同时都非常高。 但实际上这两个指标是一对矛盾体，无法做到双高。图中明显看到，如果其中一个非常高，另一个肯定会非常低。选取合适的阈值点要根据实际需求，比如我们想要高的查全率，那么我们就会牺牲一些查准率，在保证查全率最高的情况下，查准率也不那么低。

在进行性能比较时，如果一个学习器的曲线被另一个学习器的曲线完全包住，则可断言后者性能优于前者，如图 2.3，学习器 `A` 性能优于学习器 `C`。比较 `P-R` 曲线下面积的大小，也可判别学习器的优劣，它在一定程度上表征了学习器在查准率和查全率上取得”双高“的比例，但这个值不容易估算，因此人们又设计了一些综合考虑查准率、查全率的性能度量，如”平衡点（`Break-Event Point`，简称 `BEP`）“。

`BEP` 是”查准率=查全率“时的取值，基于学习器的比较，可以认为学习器 `A` 优于 `B`。但 `BEP` 过于简单了，更为常用的是 $F1$ 度量。

$$F1 = \frac{2\times P\times R}{P+R} = \frac{2\times TP}{样例总数+TP-TN}$$

$F1$ 度量的一般形式：$F_{\beta}$，能让我们表达出对查准率/查全率的偏见，$F_{\beta}$ 计算公式如下：

$$F_{\beta} = \frac{1+\beta^{2}\times P\times R}{(\beta^{2}\times P)+R}$$

其中 $\beta >1$ 对查全率有更大影响，$\beta < 1$ 对查准率有更大影响。

很多时候我们会有多个混淆矩阵，例如进行多次训练/测试，每次都能得到一个混淆矩阵；或者是在多个数据集上进行训练/测试，希望估计算法的”全局“性能；又或者是执行多分类任务，**每两两类别**的组合都对应一个混淆矩阵；....总而来说，我们希望能在 $n$ 个二分类混淆矩阵上综合考虑查准率和查全率。

一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率，记为$(P_1,R_1),(P_2,R_2),...,(P_n,R_n)$然后取平均，这样得到的是”宏查准率（`Macro-P`）“、”宏查准率（`Macro-R`）“及对应的”宏$F1$（`Macro-F1`）“：
$$Macro\ P = \frac{1}{n}\sum_{i=1}^{n}P_i$$
$$Macro\ R = \frac{1}{n}\sum_{i=1}^{n}R_i$$
$$Macro\ F1 = \frac{2 \times Macro\ P\times Macro\ R}{Macro\ P + Macro\ R}$$

另一种做法是将各混淆矩阵对应元素进行平均，得到 $TP、FP、TN、FN$ 的平均值，再基于这些平均值计算出”微查准率“（`Micro-P`）、”微查全率“（`Micro-R`）和”微$F1$“（`Mairo-F1`）

$$Micro\ P = \frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$
$$Micro\ R = \frac{\overline{TP}}{\overline{TP}+\overline{FN}}$$
$$Micro\ F1 = \frac{2 \times Micro\ P\times Micro\ R}{MacroP+Micro\ R}$$

#### 2.3.3 ROC 与 AUC

`ROC` 曲线示意图如下。
![PR曲线图](../data/images/ml/ROC曲线.png)

### 2.5 偏差与方差

通过前面的学习，我们已经知道如何设计实验（训练集、验证集的划分）来**对学习器的泛化误差进行评估**，并且也了解了诸如**精度、查准率、查全率及 `F1`** 等性能度量指标。但这不够，我们还希望了解”为什么“具有这样的性能。”偏差-方差分解“（`bias-variance decomposition`）是解释学习算法泛化性能的一种重要工具。

假设对于测试样本 $x$,令 $y_D$ 为 $x$ 在数据集中的标记（可能会有噪声），$y$ 才是 $x$ 的真实标记，$f(x;D)$ 为训练集 $D$ 上学得模型 $f$ 在 $x$ 上的预测输出。以回归任务为例，**算法的泛化误差**计算如下：

$$E(f;D) = \mathbb{E}_{D}[(f(x;D)-y_{D})^2]$$

直接计算上式是不行的，我们得进行拆解，拆解之前，有必要先知道方差、偏差、噪声的计算。学习算法的期望预测为

$$\bar{f}(x) = \mathbb{E}_{D}[(f(x;D)]$$

使用样本数相同的不同训练集产生的方差为

$$var(x) = \mathbb{E}_{D}[(f(x;D)-\mathbb{E}_{D}[(f(x;D)])^2] = \mathbb{E}_{D}[(f(x;D)-\bar{f}(x))^2]$$

噪声为

$$\varepsilon^2 = \mathbb{E}_{D}[(y_{D}-y)^2]$$

期望输出与真实标记的差别称为偏差（`bias`），即

$$bias^2(x) = (\bar{f}(x)-y)^2$$

假定噪声为 `0` ，即 $\mathbb{E}_{D}[y_{D}-y]=0$，有了以上定义，通过多项式展开合并，并利用恒等变形、期望的运算性质可将期望泛化误差公式进行分解得到：
> 公式推理证明，可参考[这里](https://datawhalechina.github.io/pumpkin-book/#/chapter2/chapter2?id=_241)。

$$E(f;D) = \mathbb{E}_{D}[(f(x;D)-\bar{f}(x))^2] + (\bar{f}(x)-y)^2 + \mathbb{E}_{D}[(y_{D}-y)^2]$$

于是，

$$ E(f;D) = var(x) + bias^2(x) + \varepsilon^2$$

通过上式，可知**泛化误差可分解为方差、偏差与噪声之和**。回顾偏差、方差、噪声的定义：

+ 偏差：度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的拟合能力。
+ 方差：度量了同样大小的训练集的变动导致的学习性能的变化，刻画了数据扰动所造成的影响，或者说刻画了模型的稳定性和泛化能力。
+ 噪声：表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

通过对泛化误差进行分解说明，模型泛化性能是由学习算法的能力、数据的充分性以及任务本身的难度所共同决定的。**模型欠拟合**表现为偏差较大, 此时偏差主导了泛化错误率；**模型过拟合**表现为偏差小但是方差很大，方差主导了泛化错误率。

一般来说，偏差与方差是有冲突的，这称为偏差-方差窘境（`bias-variane dilemma`）。

![泛化误差与偏差方差的关系示意图](../data/images/ml/泛化误差与偏差方差的关系示意图.png)
## 第 9 章 聚类

### 9.1 聚类任务

在“无监督学习”中，训练样本是无标签信息的，目标是通过对无标记训练样本的学习来揭示数据的内在性质和规律，为进一步的数据分析提供基础。这类学习任务中应用最广的就是“聚类”（`clustering`）算法，其他的无监督学习任务还有密度估计（`density estimation`）、异常检测（`anomaly detection`）等。

聚类的任务是将数据集中的样本划分为若干个通常是不相交的**子集**，每个子集称为一个“簇”（`cluster`），簇所对应的概念和语义由使用者来把握。

聚类可用作其他学习任务的一个前驱过程。基于不同的学习策略，有着多种类型的聚类算法，聚类算法涉及的两个基本问题是性能度量和距离计算。

### 9.2 性能度量

聚类性能度量也叫聚类“有效性指标”（`validity index`），和监督学习的性能度量作用类似，都是用来评估算法的好坏，同时也是聚类过程中的优化目标。

聚类性能度量大致有两类。一类是将聚类结果与某个“参考模型（`reference model`）”进行比较，称为“外部指标（`external index`）”；另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”（`inderna index`）。

外部指标包括 `Jaccard` 指数（简称 `JC`）、`FM` 指数（简称 `FMI`）和 `Rand` 指数（简称 `RI`），范围在 `[0,1]` 之间，且越大越好；内部指标包括 `DB` 指数（简称 `DBI`）和 `Dunn` 指数（简称 `DI`），`DBI` 值越小越好，`DI` 越大越好。
> 具体计算公式太多了，这里不给出，可以参考原书 `199` 页。

### 9.3 距离计算

函数 $dist(\cdot,\cdot)$ 用于计算两个样本之间的距离，如果它是一个“距离度量”（`distance measure`），则需满足一些基本性质：
+ 非负性：$dist(x_i,x_j) \geq 0$；
+ 同一性：$dist(x_i,x_j)=0$ 当前仅当 $x_i = x_j$；
+ 对称性：$dist(x_i,x_j) = dist(x_j,x_i)$；
+ 直递性：$dist(x_i,x_j \geq dist(x_i,x_k) + dist(x_k,x_j))$。

给定样本 $x_i = (x_{i1};x_{i2};...;x_{in})$ 与 $x_j = (x_{j1};x_{j2};...;x_{jn})$，最常用的是“闵可夫斯距离”（`Minkowski distance`）。

$$dist_{mk}(x_i,x_j) = (\sum_{u=1}^{n}\left | x_{iu} - x_{ju} \right |^p)^\frac{1}{p}$$

上式其实就是 $x_i - x_j$ 的 $L_p$ 范数 $\left \| x_i - x_j \right \|_p$。$p = 2$，上式为欧氏距离。 

属性通常划分为“连续属性”（`continuous attribute`）和“离散属性”（`categotical attribute`），前者在定义域上有无穷多个可能的取值，后者在定义域上是有限个取值。但是在涉及距离计算时，属性上定义了“序”更为重要。能直接在属性上计算距离：“1” 和 “2” 比较近、和“4”比较远，这样的属性称为“有序属性”（`ordinal attribute` ）；而定义域为{汽车、人、椅子}这样的离散属性则不能直接在属性上计算距离，称为“无序属性”（`non-ordinal atribute`）。显然，**闵可夫斯距离可用于有序属性**。

对无序属性采用 `VDM`（`Value Difference Metric`），同时将闵可夫斯距离和 `VDM` 集合可处理混合属性。

### 9.4 原型聚类

原型聚类算法假设聚类结构能通过一组原型刻画，这里的原型是指样本空间中具有代表性的点。一般算法先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型表示、不同的求解方式，将产生不同的算法。

#### 9.4.1 k 均值算法

`K-Means` 算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为 $K$ 个簇，让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。

给定样本集 $D = \{x_1, x_2,...,x_m\}$，假设簇划分为 $C = \{C_1,C_2,...,C_m\}$，算法目标是最小化平方误差。

$$E = \sum_{i=1}^{k}\sum_{x \in C_{i}}\left \| x - u_i \right \|_2^2$$

其中 $u_i=\frac{1}{C_i}\sum_{x \in C_{i}}x$ 是簇 $C_i$ 的均值向量，也称质心。上式一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，$E$ 值越小簇内样本相似程度越高。

找到 $E$ 的最优解需要靠擦样本集 $D$ 所有可能的簇划分，这是一个 $NP$ 难问题。$k$ 均值算法采用了贪心策略，通过迭代优化近似求解，算法流程如图 `9.2` 所示。其中第 `1` 行对均值向量进行初始化，`4-8` 行与 `9-16` 行依次对当前簇划分即均值向量迭代更新，若更新后聚类结果保持不变，则在第 `18` 行将当前簇划分结果返回。

![k-means算法流程](../data/images/k-means.png)

参考此[文章](https://www.pythonf.cn/read/116646)的代码，我修改为如下精简版的 `K-Means` 算法代码。

```python
def kmeans(dataset, k):
    """K-means 聚类算法

    Args:
        dataset ([ndarray]): 数据集，二维数组
        k ([int]): 聚簇数量
    """
    m = np.shape(dataset)[0]  # 样本个数
    
    # 1, 随机初始化聚类中心点
    center_indexs = random.sample(range(m), k)
    center = dataset[center_indexs,:]
    
    cluster_assessment = np.zeros((m, 2))
    cluster_assessment[:, 0] = -1  # 将所有的类别置为 -1
    cluster_changed = True 
    while cluster_changed:
        cluster_changed = False
        # 4-8，计算样本x_i与各聚类中心的距离，根据距离最近的聚类中心确定x_j的簇标记，并将对应样本x_i划入相应的簇
        for i in range(m):
            # 初始化样本和聚类中心的距离，及样本对应簇
            min_dist = inf
            c = 0
            # 确定每一个样本离哪个中心点最近，和属于哪一簇
            for j in range(k):
                dist = distEclud(dataset[i,:], center[j,:])
                if dist < min_dist:
                    min_dist = dist
                    c = i
            # 更新样本所属簇
            if cluster_assessment[i, 0] != c:  # 仍存在数据在前后两次计算中有类别的变动，未达到迭代停止要求
                cluster_assessment[i, :] = c, min_dist  # 更新样本所属簇
                cluster_changed = True
        # 9-16 更新簇中心点位置
        for j in range(k):
            changed_center = dataset[cluster_assessment[:,0] == j].mean(axis=0)
            center[j,:] = changed_center
            
    return cluster_assessment, center

if __name__ == '__main__':
    x1 = np.random.randint(0, 50, (50, 2))
    x2 = np.random.randint(40, 100, (50, 2))
    x3 = np.random.randint(90, 120, (50, 2))
    x4 = np.random.randint(110, 160, (50, 2))
    test = np.vstack((x1, x2, x3, x4))

    # 对特征进行聚类
    result, center = kmeans(test, 4, is_kmeans=False, is_random=False)
    print(center) # 打印簇类中心点
```

## 参考资料
《机器学习》-周志华